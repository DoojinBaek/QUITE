{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment resources related to the QUITE corpus (EMNLP 2024).\n",
    "\n",
    "Copyright (c) 2024 Robert Bosch GmbH\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published\n",
    "by the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to add this repo to the PYTHONPATH env variable!\n",
    "from src.constants import QUITE_Config\n",
    "from src.experiments.src.numeric_evaluator import NumericEvaluator\n",
    "from src.utils.quite_dataset_loaders import quite_dataset_mappings\n",
    "from datasets import Split, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always predict 50% likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions: list[float] = []\n",
    "ground_truths: list[float] = []\n",
    "\n",
    "ne: NumericEvaluator = NumericEvaluator()\n",
    "\n",
    "quite_test_dataset: Dataset = quite_dataset_mappings[Split.TEST][QUITE_Config.EVIDENCE_QUERY_PAIRS.value]\n",
    "\n",
    "for row in quite_test_dataset:\n",
    "    if row[\"type\"] == \"query\":\n",
    "        predictions.append(0.5)\n",
    "        ground_truths.append(row[\"answer\"])\n",
    "\n",
    "results: dict[str, float] = ne.get_metrics(predicted_probs=predictions, true_probs=ground_truths, reasoning_types=[[\"causal\"] for _ in range(len(predictions))])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always predict average probability of train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs: list[float] = []\n",
    "results = None\n",
    "\n",
    "quite_train_dataset: Dataset = quite_dataset_mappings[Split.TRAIN][QUITE_Config.EVIDENCE_QUERY_PAIRS.value]\n",
    "quite_test_dataset: Dataset = quite_dataset_mappings[Split.TEST][QUITE_Config.EVIDENCE_QUERY_PAIRS.value]\n",
    "\n",
    "for row in quite_train_dataset:\n",
    "    if row[\"type\"] == \"query\":\n",
    "        train_probs.append(row[\"answer\"])\n",
    "\n",
    "avg_prob: float = np.average(train_probs)\n",
    "\n",
    "print(avg_prob)\n",
    "\n",
    "predictions: list[float] = []\n",
    "ground_truths: list[float] = []\n",
    "for row in quite_test_dataset:\n",
    "    if row[\"type\"] == \"query\":\n",
    "        predictions.append(avg_prob)\n",
    "        ground_truths.append(row[\"answer\"])\n",
    "\n",
    "results = ne.get_metrics(predicted_probs=predictions, true_probs=ground_truths, reasoning_types=[[\"causal\"] for _ in range(len(predictions))])\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
